{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to import\n",
      "Done Import\n"
     ]
    }
   ],
   "source": [
    "# !pip install rank_bm25 pythainlp\n",
    "import os  \n",
    "try : \n",
    "    print(\"Try to import\")\n",
    "    import requests\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "except:\n",
    "    print(\"Download Env\")\n",
    "    os.system(\"pip install rank_bm25 pythainlp pandas\")\n",
    "    import pandas as pd \n",
    "    import requests\n",
    "    from tqdm import tqdm\n",
    "print(\"Done Import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QueryTyphoon_70b(Prompt,device='cuda',token=512,temp=0.3):\n",
    "    endpoint = 'https://api.opentyphoon.ai/v1/chat/completions'\n",
    "    res = requests.post(endpoint, json={\n",
    "        \"model\": \"typhoon-v1.5x-70b-instruct\",\n",
    "        \"max_tokens\": token,\n",
    "        \"messages\":  [{\"content\": Prompt ,\"role\":\"user\"}],\n",
    "        \"temperature\": temp,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 0,\n",
    "        \"repetition_penalty\": 1.05,\n",
    "        \"min_p\": 0\n",
    "    }, headers={\n",
    "        \"Authorization\": \"Bearer sk-CuCbKpGo8mspCTGWEVkSXoDCTOt06BSnB5pH6RwxIZgeGCCm\",\n",
    "    })\n",
    "    data = res.json()\n",
    "    queries = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    # queries = queries.replace(\"%d.\",\"\")\n",
    "    return queries\n",
    "\n",
    "def ReturnJsonFormat(Text_Json_format,replace=False):\n",
    "    if replace:\n",
    "        Text_Json_format = Text_Json_format.replace(\"'\",'\"').replace('[','}').replace(']','}')\n",
    "    Text_Json_format = Text_Json_format.replace(\"'\",'\"')\n",
    "    return json.loads(Text_Json_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retireval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = ['Python1',\n",
    "          'Python2',\n",
    "          'Python3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query_Topic = 'เรียนPython'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25 = []\n",
    "Questionlist = []\n",
    "tokenized_corpus = [word_tokenize(doc, engine=\"newmm\") for doc in Corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02583958 -0.02583958 -0.02583958]\n",
      "เรียนPython,['Python3']\n"
     ]
    }
   ],
   "source": [
    "# for i in Question:\n",
    "#   query = i\n",
    "tokenized_query = word_tokenize(Query_Topic, engine=\"newmm\")\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "ans = bm25.get_top_n(tokenized_query,Corpus, n=1)\n",
    "print(doc_scores)\n",
    "Questionlist.append(Query_Topic)\n",
    "BM25.append(ans)\n",
    "print(f\"{Query_Topic},{ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 Create content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python Programming</td>\n",
       "      <td>เรียนรู้ภาษา Python พื้นฐาน เช่น การเขียนฟังก์...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep Learning Concepts</td>\n",
       "      <td>เข้าใจหลักการพื้นฐานของ Deep Learning เช่น Neu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "      <td>ศึกษาเกี่ยวกับการประมวลผลภาษาธรรมชาติ (NLP) เช...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformer Architecture</td>\n",
       "      <td>เข้าใจโครงสร้าง Transformer ที่ใช้ในการสร้าง C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hugging Face Transformers Library</td>\n",
       "      <td>เรียนรู้การใช้งาน Hugging Face Transformers Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fine-tuning Pre-trained Models</td>\n",
       "      <td>ศึกษาเกี่ยวกับการปรับปรุงโมเดลที่ถูกฝึกอบรมมาแ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Topic  \\\n",
       "0                 Python Programming   \n",
       "1             Deep Learning Concepts   \n",
       "2  Natural Language Processing (NLP)   \n",
       "3           Transformer Architecture   \n",
       "4  Hugging Face Transformers Library   \n",
       "5     Fine-tuning Pre-trained Models   \n",
       "\n",
       "                                              detail  \n",
       "0  เรียนรู้ภาษา Python พื้นฐาน เช่น การเขียนฟังก์...  \n",
       "1  เข้าใจหลักการพื้นฐานของ Deep Learning เช่น Neu...  \n",
       "2  ศึกษาเกี่ยวกับการประมวลผลภาษาธรรมชาติ (NLP) เช...  \n",
       "3  เข้าใจโครงสร้าง Transformer ที่ใช้ในการสร้าง C...  \n",
       "4  เรียนรู้การใช้งาน Hugging Face Transformers Li...  \n",
       "5  ศึกษาเกี่ยวกับการปรับปรุงโมเดลที่ถูกฝึกอบรมมาแ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Topic = \"สร้างเเชทGPT\"\n",
    "Prompt_Task1 = f\"\"\"You are a planner teacher for student learning. The Student want to learn how to {Topic}. So Create a guideline that he should study skill or knowledge before {Topic} \n",
    "write the Answer in the Json format and Thai Language. And do not mention to the title of content just only Json format for the response \n",
    "\n",
    "'Learning1' : ['Topic' :'q1',\n",
    "                    'detail' : 'd1'\n",
    "                ],\n",
    "                    \n",
    "'Learning2' : 'Topic' :'q2',\n",
    "                'detail' : 'd2'\n",
    "                ]\n",
    "\"\"\"\n",
    "\n",
    "answer_task1 = QueryTyphoon_70b(Prompt_Task1,token=512) #Query 1 \n",
    "queries_task1 = ReturnJsonFormat(answer_task1,replace=True)\n",
    "df_task1 = pd.DataFrame([\n",
    "    {\"Topic\": q_info['Topic'],\"detail\": q_info['detail']}\n",
    "    for q_key, q_info in queries_task1.items()\n",
    "])\n",
    "df_task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Programming\n",
      "Deep Learning Concepts\n",
      "Natural Language Processing (NLP)\n",
      "Transformer Architecture\n",
      "Hugging Face Transformers Library\n",
      "Fine-tuning Pre-trained Models\n"
     ]
    }
   ],
   "source": [
    "value = \"\\n\".join(map(str, df_task1['Topic'].values))\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a greatest teacher create a exercies to meansure knowledge of student, Your work is create a 10 question student who plan to สร้างเเชทGPT and So you need to create a question for Level measurement knowledge of this student in each Topic that he need to learn.\n",
      "This is topic that he need to learn for สร้างเเชทGPT\n",
      "```\n",
      "Python Programming\n",
      "Deep Learning Concepts\n",
      "Natural Language Processing (NLP)\n",
      "Transformer Architecture\n",
      "Hugging Face Transformers Library\n",
      "Fine-tuning Pre-trained Models\n",
      "```\n",
      "\n",
      "And have a Detail about question for meansure student. You need to follow these 4 Rule \n",
      "1. You have to create 10 question and then return me with Format that I give below\n",
      "2. that question is about สร้างเเชทGPT and that question gonna meansure knowledge of student in topic that i give you before\n",
      "3. Do not Answer the tile of this about just only return of question that you created\n",
      "4. After Question will have a Topic that question about \n",
      "```\n",
      "1. (Question1) (Topic)\n",
      "2. (Question2)\n",
      ".\n",
      "#repeat this process untill 10 question\n",
      ".\n",
      "10. (Question10)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Task \n",
    "Topics = \"เรียน Python\"\n",
    "Time = \"24 ชั่วโมง\"\n",
    "Prompt_Task2 = f\"\"\"\n",
    "You are a greatest teacher create a exercies to meansure knowledge of student, Your work is create a 10 question student who plan to {Topics} and So you need to create a question for Level measurement knowledge of this student in each Topic that he need to learn.\n",
    "This is topic that he need to learn for {Topics}\n",
    "```\n",
    "{value}\n",
    "```\n",
    "\n",
    "And have a Detail about question for meansure student. You need to follow these 4 Rule \n",
    "1. You have to create 10 question and then return me with Format that I give below\n",
    "2. that question is about {Topics} and that question gonna meansure knowledge of student in topic that i give you before\n",
    "3. Do not Answer the tile of this about just only return of question that you created\n",
    "4. After Question will have a Topic that question about \n",
    "```\n",
    "1. (Question1) (Topic)\n",
    "2. (Question2)\n",
    ".\n",
    "#repeat this process untill 10 question\n",
    ".\n",
    "10. (Question10)\n",
    "```\n",
    "\"\"\"\n",
    "print(Prompt_Task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Write a Python script to preprocess text data by tokenizing, converting to lowercase, and removing stopwords using the NLTK library. (Python Programming)\n",
      "2. Explain the concept of attention mechanism in deep learning and how it's used in transformer models. (Deep Learning Concepts)\n",
      "3. What is the difference between word embeddings and position embeddings in the context of NLP? Provide examples of each. (Natural Language Processing (NLP))\n",
      "4. Describe the main components of the BERT architecture, including the input representation, encoder, and pooler layers. (Transformer Architecture)\n",
      "5. How can you use the Hugging Face Transformers library to load a pre-trained BERT model and fine-tune it on your own dataset? Provide code snippets. (Hugging Face Transformers Library)\n",
      "6. What are some common techniques for fine-tuning pre-trained models, such as BERT, on specific NLP tasks like sentiment analysis or question-answering? (Fine-tuning Pre-trained Models)\n",
      "7. Write a Python function to calculate the perplexity of a given text corpus using a pre-trained GPT model. (Python Programming)\n",
      "8. Explain the concept of masked language modeling and how it's used in the pre-training phase of BERT-like models. (Deep Learning Concepts)\n",
      "9. How does the GPT model handle out-of-vocabulary words during inference? Describe a strategy to improve its performance on unseen words. (Natural Language Processing (NLP))\n",
      "10. Compare and contrast the architectures of GPT and BERT, highlighting their similarities and differences. (Transformer Architecture)\n"
     ]
    }
   ],
   "source": [
    "query_task2 = QueryTyphoon_70b(Prompt_Task2,temp=0.6)\n",
    "print(query_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a greatest teacher for create 10 exercises, Your work to create translate question that i give and create a Choice in 10 question for test student who plan to สร้างเเชทGPT to Level measurement knowledge of this student that what level there he is. You have a skill that know 2 language in english and thai language and you very good at it \n",
      "The Detail about question for meansure student So I have 5 Rule you should follow: \n",
      "1. I have a 10 question that you need to traslate to Thai language and then create choice that in thai langguage with Json Format (I give below)\n",
      "2. that question is about สร้างเเชทGPT\n",
      "3. In each question have 4 choice and 3 each choice is a correct answer,expect 1 choice So there all 3 choice each will have a personal point like 0.25 0.5 0.75 and 1 Then store data tuple and in Choice1-4 in Choice \n",
      "4. In each question will have Topic that about So write the Topic that about in Json Format in Topic\n",
      "5. **This is priority number1** question and choice that create gonna be in Thai language\n",
      "write the Answer only in the Json format and English Language, Do not add the Title what is this Json for \n",
      "\n",
      "Here is a 10 question that you gonna translate and create choice \n",
      "```\n",
      "1. Write a Python script to preprocess text data by tokenizing, converting to lowercase, and removing stopwords using the NLTK library. (Python Programming)\n",
      "2. Explain the concept of attention mechanism in deep learning and how it's used in transformer models. (Deep Learning Concepts)\n",
      "3. What is the difference between word embeddings and position embeddings in the context of NLP? Provide examples of each. (Natural Language Processing (NLP))\n",
      "4. Describe the main components of the BERT architecture, including the input representation, encoder, and pooler layers. (Transformer Architecture)\n",
      "5. How can you use the Hugging Face Transformers library to load a pre-trained BERT model and fine-tune it on your own dataset? Provide code snippets. (Hugging Face Transformers Library)\n",
      "6. What are some common techniques for fine-tuning pre-trained models, such as BERT, on specific NLP tasks like sentiment analysis or question-answering? (Fine-tuning Pre-trained Models)\n",
      "7. Write a Python function to calculate the perplexity of a given text corpus using a pre-trained GPT model. (Python Programming)\n",
      "8. Explain the concept of masked language modeling and how it's used in the pre-training phase of BERT-like models. (Deep Learning Concepts)\n",
      "9. How does the GPT model handle out-of-vocabulary words during inference? Describe a strategy to improve its performance on unseen words. (Natural Language Processing (NLP))\n",
      "10. Compare and contrast the architectures of GPT and BERT, highlighting their similarities and differences. (Transformer Architecture)\n",
      "```\n",
      "This is Json format that you need to duel with \n",
      "{\n",
      "\"Question1\" : {\"Question\" : (q1),\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\n",
      "                            (c1),(weight point1)\n",
      "                            ],\n",
      "                        \"Choice2\" : [\n",
      "                            (c2),(weight point2)\n",
      "                            ],\n",
      "                        \"Choice3\" : [\n",
      "                            (c3),(weight point3)\n",
      "                            ],\n",
      "                        \"Choice4\" : [\n",
      "                            (c4),(weight point4)\n",
      "                            ],\n",
      "                            },\n",
      "                    \"Topic\" : \"(Topic about question)\"\n",
      "                    },\n",
      "\"Question2\" : {\"Question\" : (q1),\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\n",
      "                            (c1),(weight point1)\n",
      "                            ],\n",
      "                        \"Choice2\" : [\n",
      "                            (c2),(weight point2)\n",
      "                            ],\n",
      "                        \"Choice3\" : [\n",
      "                            (c3),(weight point3)\n",
      "                            ],\n",
      "                        \"Choice4\" : [\n",
      "                            (c4),(weight point4)\n",
      "                            ],\n",
      "                            },\n",
      "                    \"Topic\" : \"(Topic about question)\"\n",
      "                    },\n",
      "}\n",
      "I warning you again that all the question and answer gonna be in Thai language only\n"
     ]
    }
   ],
   "source": [
    "# def ReturnPrompt(query_task2_1,Topic=Topics):\n",
    "Prompt_Task2_2 = f\"\"\"You are a greatest teacher for create 10 exercises, Your work to create translate question that i give and create a Choice in 10 question for test student who plan to {Topic} to Level measurement knowledge of this student that what level there he is. You have a skill that know 2 language in english and thai language and you very good at it \n",
    "The Detail about question for meansure student So I have 5 Rule you should follow: \n",
    "1. I have a 10 question that you need to traslate to Thai language and then create choice that in thai langguage with Json Format (I give below)\n",
    "2. that question is about {Topic}\n",
    "3. In each question have 4 choice and 3 each choice is a correct answer,expect 1 choice So there all 3 choice each will have a personal point like 0.25 0.5 0.75 and 1 Then store data tuple and in Choice1-4 in Choice \n",
    "4. In each question will have Topic that about So write the Topic that about in Json Format in Topic\n",
    "5. **This is priority number1** question and choice that create gonna be in Thai language\n",
    "write the Answer only in the Json format and English Language, Do not add the Title what is this Json for \n",
    "\n",
    "Here is a 10 question that you gonna translate and create choice \n",
    "```\n",
    "{query_task2}\n",
    "```\n",
    "This is Json format that you need to duel with \n",
    "{{\n",
    "\"Question1\" : {{\"Question\" : (q1),\n",
    "                    \"Choice\" : {{\n",
    "                        \"Choice1\" : [\n",
    "                            (c1),(weight point1)\n",
    "                            ],\n",
    "                        \"Choice2\" : [\n",
    "                            (c2),(weight point2)\n",
    "                            ],\n",
    "                        \"Choice3\" : [\n",
    "                            (c3),(weight point3)\n",
    "                            ],\n",
    "                        \"Choice4\" : [\n",
    "                            (c4),(weight point4)\n",
    "                            ],\n",
    "                            }},\n",
    "                    \"Topic\" : \"(Topic about question)\"\n",
    "                    }},\n",
    "\"Question2\" : {{\"Question\" : (q1),\n",
    "                    \"Choice\" : {{\n",
    "                        \"Choice1\" : [\n",
    "                            (c1),(weight point1)\n",
    "                            ],\n",
    "                        \"Choice2\" : [\n",
    "                            (c2),(weight point2)\n",
    "                            ],\n",
    "                        \"Choice3\" : [\n",
    "                            (c3),(weight point3)\n",
    "                            ],\n",
    "                        \"Choice4\" : [\n",
    "                            (c4),(weight point4)\n",
    "                            ],\n",
    "                            }},\n",
    "                    \"Topic\" : \"(Topic about question)\"\n",
    "                    }},\n",
    "}}\n",
    "I warning you again that all the question and answer gonna be in Thai language only\"\"\"\n",
    "# return Prompt_Task2_2\n",
    "print(Prompt_Task2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed in 89.5736s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Choice1</th>\n",
       "      <th>Choice2</th>\n",
       "      <th>Choice3</th>\n",
       "      <th>Choice4</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>เขียนสคริปต์ Python เพื่อทำการปรับแต่งข้อมูลข้...</td>\n",
       "      <td>(การแบ่งคำ, 0.25)</td>\n",
       "      <td>(การแปลงเป็นตัวเล็ก, 0.5)</td>\n",
       "      <td>(การลบคำหยุด, 0.75)</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>อธิบายแนวคิดของกลไกการสนใจใน deep learning และ...</td>\n",
       "      <td>(การปรับปรุงประสิทธิภาพ, 0.25)</td>\n",
       "      <td>(การเรียนรู้จากข้อมูล, 0.5)</td>\n",
       "      <td>(การสร้างความสัมพันธ์ระหว่างคำ, 0.75)</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Deep Learning Concepts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ความแตกต่างระหว่าง word embeddings และ positio...</td>\n",
       "      <td>(การแทนคำด้วยเวกเตอร์, 0.25)</td>\n",
       "      <td>(การแทนตำแหน่งของคำด้วยเวกเตอร์, 0.5)</td>\n",
       "      <td>(การสร้างความสัมพันธ์ระหว่างคำ, 0.75)</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>บรรยายหลักการทำงานของ BERT architecture รวมถึง...</td>\n",
       "      <td>(การแปลงข้อมูลเป็นเวกเตอร์, 0.25)</td>\n",
       "      <td>(การสร้างความสัมพันธ์ระหว่างคำ, 0.5)</td>\n",
       "      <td>(การรวมผลลัพธ์จากหลาย layer, 0.75)</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Transformer Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>คุณสามารถใช้ไลบรารี Hugging Face Transformers ...</td>\n",
       "      <td>(การโหลดโมเดล, 0.25)</td>\n",
       "      <td>(การปรับปรุงโมเดล, 0.5)</td>\n",
       "      <td>(การฝึกฝนโมเดลด้วยเซ็ตข้อมูล, 0.75)</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Hugging Face Transformers Library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>เทคนิคใดบ้างที่ใช้ในการปรับปรุงโมเดลที่ได้รับก...</td>\n",
       "      <td>(การปรับปรุง hyperparameters, 0.25)</td>\n",
       "      <td>(การเพิ่มข้อมูลที่เฉพาะเจาะจง, 0.5)</td>\n",
       "      <td>(การปรับปรุงโมเดลด้วยเทคนิค transfer learning,...</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Fine-tuning Pre-trained Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>เขียนฟังก์ชัน Python เพื่อคำนวณ perplexity ของ...</td>\n",
       "      <td>(การคำนวณความซับซ้อน, 0.25)</td>\n",
       "      <td>(การคำนวณความไม่แน่นอน, 0.5)</td>\n",
       "      <td>(การคำนวณความสับสน, 0.75)</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Python Programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>อธิบายแนวคิดของ masked language modeling และวิ...</td>\n",
       "      <td>(การฝึกฝนโมเดลด้วยข้อมูลที่มีการปกปิดบางส่วน, ...</td>\n",
       "      <td>(การสร้างความสัมพันธ์ระหว่างคำ, 0.5)</td>\n",
       "      <td>(การปรับปรุงโมเดลด้วยเทคนิค transfer learning,...</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Deep Learning Concepts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>โมเดล GPT จัดการกับคำที่ไม่มีอยู่ใน dictionary...</td>\n",
       "      <td>(การสร้างคำใหม่, 0.25)</td>\n",
       "      <td>(การปรับปรุงโมเดลด้วยเทคนิค transfer learning,...</td>\n",
       "      <td>(การเพิ่มข้อมูลที่เฉพาะเจาะจง, 0.75)</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>เปรียบเทียบและต่างกันระหว่างสถาปัตยกรรมของ GPT...</td>\n",
       "      <td>(การสร้างความสัมพันธ์ระหว่างคำ, 0.25)</td>\n",
       "      <td>(การแปลงข้อมูลเป็นเวกเตอร์, 0.5)</td>\n",
       "      <td>(การปรับปรุงโมเดลด้วยเทคนิค transfer learning,...</td>\n",
       "      <td>(ทั้งหมดข้างต้น, 1)</td>\n",
       "      <td>Transformer Architecture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  เขียนสคริปต์ Python เพื่อทำการปรับแต่งข้อมูลข้...   \n",
       "1  อธิบายแนวคิดของกลไกการสนใจใน deep learning และ...   \n",
       "2  ความแตกต่างระหว่าง word embeddings และ positio...   \n",
       "3  บรรยายหลักการทำงานของ BERT architecture รวมถึง...   \n",
       "4  คุณสามารถใช้ไลบรารี Hugging Face Transformers ...   \n",
       "5  เทคนิคใดบ้างที่ใช้ในการปรับปรุงโมเดลที่ได้รับก...   \n",
       "6  เขียนฟังก์ชัน Python เพื่อคำนวณ perplexity ของ...   \n",
       "7  อธิบายแนวคิดของ masked language modeling และวิ...   \n",
       "8  โมเดล GPT จัดการกับคำที่ไม่มีอยู่ใน dictionary...   \n",
       "9  เปรียบเทียบและต่างกันระหว่างสถาปัตยกรรมของ GPT...   \n",
       "\n",
       "                                             Choice1  \\\n",
       "0                                  (การแบ่งคำ, 0.25)   \n",
       "1                     (การปรับปรุงประสิทธิภาพ, 0.25)   \n",
       "2                       (การแทนคำด้วยเวกเตอร์, 0.25)   \n",
       "3                  (การแปลงข้อมูลเป็นเวกเตอร์, 0.25)   \n",
       "4                               (การโหลดโมเดล, 0.25)   \n",
       "5                (การปรับปรุง hyperparameters, 0.25)   \n",
       "6                        (การคำนวณความซับซ้อน, 0.25)   \n",
       "7  (การฝึกฝนโมเดลด้วยข้อมูลที่มีการปกปิดบางส่วน, ...   \n",
       "8                             (การสร้างคำใหม่, 0.25)   \n",
       "9              (การสร้างความสัมพันธ์ระหว่างคำ, 0.25)   \n",
       "\n",
       "                                             Choice2  \\\n",
       "0                          (การแปลงเป็นตัวเล็ก, 0.5)   \n",
       "1                        (การเรียนรู้จากข้อมูล, 0.5)   \n",
       "2              (การแทนตำแหน่งของคำด้วยเวกเตอร์, 0.5)   \n",
       "3               (การสร้างความสัมพันธ์ระหว่างคำ, 0.5)   \n",
       "4                            (การปรับปรุงโมเดล, 0.5)   \n",
       "5                (การเพิ่มข้อมูลที่เฉพาะเจาะจง, 0.5)   \n",
       "6                       (การคำนวณความไม่แน่นอน, 0.5)   \n",
       "7               (การสร้างความสัมพันธ์ระหว่างคำ, 0.5)   \n",
       "8  (การปรับปรุงโมเดลด้วยเทคนิค transfer learning,...   \n",
       "9                   (การแปลงข้อมูลเป็นเวกเตอร์, 0.5)   \n",
       "\n",
       "                                             Choice3              Choice4  \\\n",
       "0                                (การลบคำหยุด, 0.75)  (ทั้งหมดข้างต้น, 1)   \n",
       "1              (การสร้างความสัมพันธ์ระหว่างคำ, 0.75)  (ทั้งหมดข้างต้น, 1)   \n",
       "2              (การสร้างความสัมพันธ์ระหว่างคำ, 0.75)  (ทั้งหมดข้างต้น, 1)   \n",
       "3                 (การรวมผลลัพธ์จากหลาย layer, 0.75)  (ทั้งหมดข้างต้น, 1)   \n",
       "4                (การฝึกฝนโมเดลด้วยเซ็ตข้อมูล, 0.75)  (ทั้งหมดข้างต้น, 1)   \n",
       "5  (การปรับปรุงโมเดลด้วยเทคนิค transfer learning,...  (ทั้งหมดข้างต้น, 1)   \n",
       "6                          (การคำนวณความสับสน, 0.75)  (ทั้งหมดข้างต้น, 1)   \n",
       "7  (การปรับปรุงโมเดลด้วยเทคนิค transfer learning,...  (ทั้งหมดข้างต้น, 1)   \n",
       "8               (การเพิ่มข้อมูลที่เฉพาะเจาะจง, 0.75)  (ทั้งหมดข้างต้น, 1)   \n",
       "9  (การปรับปรุงโมเดลด้วยเทคนิค transfer learning,...  (ทั้งหมดข้างต้น, 1)   \n",
       "\n",
       "                               Topic  \n",
       "0                 Python Programming  \n",
       "1             Deep Learning Concepts  \n",
       "2  Natural Language Processing (NLP)  \n",
       "3           Transformer Architecture  \n",
       "4  Hugging Face Transformers Library  \n",
       "5     Fine-tuning Pre-trained Models  \n",
       "6                 Python Programming  \n",
       "7             Deep Learning Concepts  \n",
       "8  Natural Language Processing (NLP)  \n",
       "9           Transformer Architecture  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "t1 = time() \n",
    "query_task2_2 = QueryTyphoon_70b(Prompt_Task2_2,token=2000,temp=0.3)\n",
    "t2 = time() \n",
    "print(f'executed in {(t2-t1):.4f}s') \n",
    "\n",
    "query_task2_2_json = json.loads(query_task2_2.replace(\"'\",'\"'))\n",
    "\n",
    "rows = []\n",
    "for q_key, q_value in query_task2_2_json.items():\n",
    "    question = q_value['Question']\n",
    "    choices = q_value['Choice']\n",
    "    Topic = q_value['Topic']\n",
    "    \n",
    "    row = {\n",
    "        'Question': question,\n",
    "        'Choice1': (choices['Choice1'][0], choices['Choice1'][1]),\n",
    "        'Choice2': (choices['Choice2'][0], choices['Choice2'][1]),\n",
    "        'Choice3': (choices['Choice3'][0], choices['Choice3'][1]),\n",
    "        'Choice4': (choices['Choice4'][0], choices['Choice4'][1]),\n",
    "        'Topic': Topic\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "df_task2 = pd.DataFrame(rows)\n",
    "df_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"Question1\" : {\"Question\" : \"เขียนสคริปต์ Python เพื่อทำการปรับแต่งข้อมูลข้อความโดยการแบ่งคำ, แปลงเป็นตัวเล็ก, และลบคำหยุดการใช้ไลบรารี NLTK\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การแบ่งคำ\", 0.25],\n",
      "                        \"Choice2\" : [\"การแปลงเป็นตัวเล็ก\", 0.5],\n",
      "                        \"Choice3\" : [\"การลบคำหยุด\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Python Programming\"\n",
      "                    },\n",
      "\"Question2\" : {\"Question\" : \"อธิบายแนวคิดของกลไกการสนใจใน deep learning และวิธีการใช้งานในโมเดลทรานส์ฟอร์เมอร์\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การปรับปรุงประสิทธิภาพ\", 0.25],\n",
      "                        \"Choice2\" : [\"การเรียนรู้จากข้อมูล\", 0.5],\n",
      "                        \"Choice3\" : [\"การสร้างความสัมพันธ์ระหว่างคำ\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Deep Learning Concepts\"\n",
      "                    },\n",
      "\"Question3\" : {\"Question\" : \"ความแตกต่างระหว่าง word embeddings และ position embeddings ในบริบทของ NLP คืออะไร? ให้ตัวอย่างของแต่ละอย่าง\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การแทนคำด้วยเวกเตอร์\", 0.25],\n",
      "                        \"Choice2\" : [\"การแทนตำแหน่งของคำด้วยเวกเตอร์\", 0.5],\n",
      "                        \"Choice3\" : [\"การสร้างความสัมพันธ์ระหว่างคำ\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Natural Language Processing (NLP)\"\n",
      "                    },\n",
      "\"Question4\" : {\"Question\" : \"บรรยายหลักการทำงานของ BERT architecture รวมถึง input representation, encoder, และ pooler layers\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การแปลงข้อมูลเป็นเวกเตอร์\", 0.25],\n",
      "                        \"Choice2\" : [\"การสร้างความสัมพันธ์ระหว่างคำ\", 0.5],\n",
      "                        \"Choice3\" : [\"การรวมผลลัพธ์จากหลาย layer\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Transformer Architecture\"\n",
      "                    },\n",
      "\"Question5\" : {\"Question\" : \"คุณสามารถใช้ไลบรารี Hugging Face Transformers เพื่อโหลดโมเดล BERT ที่ได้รับการฝึกฝนแล้วและปรับปรุงให้ดีขึ้นบนเซ็ตข้อมูลของคุณเองได้อย่างไร? ให้ตัวอย่างโค้ด\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การโหลดโมเดล\", 0.25],\n",
      "                        \"Choice2\" : [\"การปรับปรุงโมเดล\", 0.5],\n",
      "                        \"Choice3\" : [\"การฝึกฝนโมเดลด้วยเซ็ตข้อมูล\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Hugging Face Transformers Library\"\n",
      "                    },\n",
      "\"Question6\" : {\"Question\" : \"เทคนิคใดบ้างที่ใช้ในการปรับปรุงโมเดลที่ได้รับการฝึกฝนแล้ว เช่น BERT บนงาน NLP ที่เฉพาะเจาะจง เช่น การวิเคราะห์ความรู้สึกหรือการตอบคำถาม?\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การปรับปรุง hyperparameters\", 0.25],\n",
      "                        \"Choice2\" : [\"การเพิ่มข้อมูลที่เฉพาะเจาะจง\", 0.5],\n",
      "                        \"Choice3\" : [\"การปรับปรุงโมเดลด้วยเทคนิค transfer learning\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Fine-tuning Pre-trained Models\"\n",
      "                    },\n",
      "\"Question7\" : {\"Question\" : \"เขียนฟังก์ชัน Python เพื่อคำนวณ perplexity ของ corpus ข้อความที่กำหนดโดยใช้โมเดล GPT ที่ได้รับการฝึกฝนแล้ว\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การคำนวณความซับซ้อน\", 0.25],\n",
      "                        \"Choice2\" : [\"การคำนวณความไม่แน่นอน\", 0.5],\n",
      "                        \"Choice3\" : [\"การคำนวณความสับสน\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Python Programming\"\n",
      "                    },\n",
      "\"Question8\" : {\"Question\" : \"อธิบายแนวคิดของ masked language modeling และวิธีการใช้งานในระยะการฝึกฝนของโมเดล BERT\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การฝึกฝนโมเดลด้วยข้อมูลที่มีการปกปิดบางส่วน\", 0.25],\n",
      "                        \"Choice2\" : [\"การสร้างความสัมพันธ์ระหว่างคำ\", 0.5],\n",
      "                        \"Choice3\" : [\"การปรับปรุงโมเดลด้วยเทคนิค transfer learning\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Deep Learning Concepts\"\n",
      "                    },\n",
      "\"Question9\" : {\"Question\" : \"โมเดล GPT จัดการกับคำที่ไม่มีอยู่ใน dictionary ระหว่างการทำนายอย่างไร? อธิบายกลยุทธ์เพื่อปรับปรุงประสิทธิภาพของโมเดลบนคำที่ไม่เคยเห็น\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การสร้างคำใหม่\", 0.25],\n",
      "                        \"Choice2\" : [\"การปรับปรุงโมเดลด้วยเทคนิค transfer learning\", 0.5],\n",
      "                        \"Choice3\" : [\"การเพิ่มข้อมูลที่เฉพาะเจาะจง\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Natural Language Processing (NLP)\"\n",
      "                    },\n",
      "\"Question10\" : {\"Question\" : \"เปรียบเทียบและต่างกันระหว่างสถาปัตยกรรมของ GPT และ BERT โดยเน้นที่ความเหมือนและความแตกต่าง\",\n",
      "                    \"Choice\" : {\n",
      "                        \"Choice1\" : [\"การสร้างความสัมพันธ์ระหว่างคำ\", 0.25],\n",
      "                        \"Choice2\" : [\"การแปลงข้อมูลเป็นเวกเตอร์\", 0.5],\n",
      "                        \"Choice3\" : [\"การปรับปรุงโมเดลด้วยเทคนิค transfer learning\", 0.75],\n",
      "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
      "                            },\n",
      "                    \"Topic\" : \"Transformer Architecture\"\n",
      "                    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(query_task2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 40 column 58 (char 2505)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m query_task2_2_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(query_task2_2)\n",
      "File \u001b[1;32mc:\\Users\\Nattapong\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\Nattapong\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\Nattapong\\anaconda3\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 40 column 58 (char 2505)"
     ]
    }
   ],
   "source": [
    "query_task2_2_json = json.loads(query_task2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"\"\"{\n",
    "\"Question1\" : {\"Question\" : \"เขียนสคริปต์ Python เพื่อทำการปรับแต่งข้อมูลข้อความโดยการแบ่งคำ, แปลงเป็นตัวเล็ก, และลบคำหยุดการใช้ห้องสมุด NLTK\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"การแบ่งคำ\", 0.25],\n",
    "                        \"Choice2\" : [\"การแปลงเป็นตัวเล็ก\", 0.25],\n",
    "                        \"Choice3\" : [\"การลบคำหยุด\", 0.25],\n",
    "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
    "                            },\n",
    "                    \"Topic\" : \"Python Programming\"\n",
    "                    },\n",
    "\"Question2\" : {\"Question\" : \"อธิบายแนวคิดของกลไกการให้ความสนใจใน deep learning และวิธีการใช้งานในโมเดลทรานซิสเตอร์\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"การให้ความสนใจเฉพาะบางส่วน\", 0.5],\n",
    "                        \"Choice2\" : [\"การปรับปรุงประสิทธิภาพในการเรียนรู้\", 0.5],\n",
    "                        \"Choice3\" : [\"การทำงานร่วมกับโมเดลทรานซิสเตอร์\", 0.5],\n",
    "                        \"Choice4\" : [\"ไม่มีข้อใดถูกต้อง\", 0]\n",
    "                            },\n",
    "                    \"Topic\" : \"Deep Learning Concepts\"\n",
    "                    },\n",
    "\"Question3\" : {\"Question\" : \"ความแตกต่างระหว่างการฝังคำและตำแหน่งการฝังคำในบริบทของ NLP คืออะไร? ให้ตัวอย่างของแต่ละอย่าง\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"การฝังคำสำหรับการแทนความหมาย\", 0.5],\n",
    "                        \"Choice2\" : [\"ตำแหน่งการฝังคำสำหรับการแทนลำดับ\", 0.5],\n",
    "                        \"Choice3\" : [\"ทั้งสองอย่างใช้ในการสร้างโมเดลภาษา\", 0.5],\n",
    "                        \"Choice4\" : [\"ไม่มีข้อใดถูกต้อง\", 0]\n",
    "                            },\n",
    "                    \"Topic\" : \"Natural Language Processing (NLP)\"\n",
    "                    },\n",
    "\"Question4\" : {\"Question\" : \"บรรยายหลักการทำงานของ BERT architecture รวมถึงชั้นการแสดงข้อมูล, ชั้นเอนโค้ดเดอร์, และชั้นพูลเลอร์\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"ชั้นการแสดงข้อมูลสำหรับการเตรียมข้อมูล\", 0.25],\n",
    "                        \"Choice2\" : [\"ชั้นเอนโค้ดเดอร์สำหรับการเรียนรู้\", 0.25],\n",
    "                        \"Choice3\" : [\"ชั้นพูลเลอร์สำหรับการสรุปผล\", 0.25],\n",
    "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
    "                            },\n",
    "                    \"Topic\" : \"Transformer Architecture\"\n",
    "                    },\n",
    "\"Question5\" : {\"Question\" : \"คุณสามารถใช้ห้องสมุด Hugging Face Transformers เพื่อโหลดโมเดล BERT ที่ได้รับการฝึกอบรมแล้วและปรับปรุงให้ดีขึ้นบนเซ็ตข้อมูลของคุณเองได้อย่างไร? ให้ตัวอย่างโค้ด\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"การใช้ฟังก์ชัน load\\_pretrained() ของ Hugging Face\", 0.5],\n",
    "                        \"Choice2\" : [\"การใช้ฟังก์ชัน fine\\_tune() ของ Hugging Face\", 0.5],\n",
    "                        \"Choice3\" : [\"การใช้โมเดล BERT ที่ได้รับการฝึกอบรมแล้ว\", 0.5],\n",
    "                        \"Choice4\" : [\"ไม่มีข้อใดถูกต้อง\", 0]\n",
    "                            },\n",
    "                    \"Topic\" : \"Hugging Face Transformers Library\"\n",
    "                    },\n",
    "\"Question6\" : {\"Question\" : \"เทคนิคใดๆ ที่ใช้ในการปรับปรุงโมเดลที่ได้รับการฝึกอบรมแล้ว เช่น BERT บนงาน NLP ที่เฉพาะเจาะจง เช่น การวิเคราะห์ความรู้สึกหรือการตอบคำถาม?\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"การใช้เทคนิค fine-tuning\", 0.33],\n",
    "                        \"Choice2\" : [\"การใช้เทคนิค transfer learning\", 0.33],\n",
    "                        \"Choice3\" : [\"การใช้เทคนิค domain adaptation\", 0.33],\n",
    "                        \"Choice4\" : [\"ไม่มีข้อใดถูกต้อง\", 0]\n",
    "                            },\n",
    "                    \"Topic\" : \"Fine-tuning Pre-trained Models\"\n",
    "                    },\n",
    "\"Question7\" : {\"Question\" : \"เขียนฟังก์ชัน Python เพื่อคำนวณความซับซ้อนของข้อมูลข้อความที่กำหนดโดยใช้โมเดล GPT ที่ได้รับการฝึกอบรมแล้ว\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"การใช้ฟังก์ชัน perplexity() ของ GPT\", 0.5],\n",
    "                        \"Choice2\" : [\"การใช้ฟังก์ชัน loss() ของ GPT\", 0.5],\n",
    "                        \"Choice3\" : [\"การใช้โมเดล GPT ที่ได้รับการฝึกอบรมแล้ว\", 0.5],\n",
    "                        \"Choice4\" : [\"ไม่มีข้อใดถูกต้อง\", 0]\n",
    "                            },\n",
    "                    \"Topic\" : \"Python Programming\"\n",
    "                    },\n",
    "\"Question8\" : {\"Question\" : \"อธิบายแนวคิดของการจำลองภาษาที่ปกคลุมและมันถูกใช้ในขั้นตอนการฝึกอบรมของโมเดล BERT\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"การจำลองภาษาที่ปกคลุมเพื่อสร้างข้อมูลที่ไม่สมบูรณ์\", 0.5],\n",
    "                        \"Choice2\" : [\"การใช้ข้อมูลที่ไม่สมบูรณ์เพื่อฝึกอบรมโมเดล\", 0.5],\n",
    "                        \"Choice3\" : [\"การจำลองภาษาที่ปกคลุมมีประโยชน์ในการสร้างโมเดลภาษา\", 0.5],\n",
    "                        \"Choice4\" : [\"ไม่มีข้อใดถูกต้อง\", 0]\n",
    "                            },\n",
    "                    \"Topic\" : \"Deep Learning Concepts\"\n",
    "                    },\n",
    "\"Question9\" : {\"Question\" : \"โมเดล GPT จัดการกับคำที่ไม่อยู่ในพจนานุกรมอย่างไรในช่วงการทำนาย? อธิบายกลยุทธ์เพื่อปรับปรุงประสิทธิภาพในการทำงานกับคำที่ไม่เคยเห็น\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"การใช้เทคนิค subwording\", 0.5],\n",
    "                        \"Choice2\" : [\"การใช้เทคนิค character-level embedding\", 0.5],\n",
    "                        \"Choice3\" : [\"การใช้เทคนิค out-of-vocabulary handling\", 0.5],\n",
    "                        \"Choice4\" : [\"ไม่มีข้อใดถูกต้อง\", 0]\n",
    "                            },\n",
    "                    \"Topic\" : \"Natural Language Processing (NLP)\"\n",
    "                    },\n",
    "\"Question10\" : {\"Question\" : \"เปรียบเทียบและต่างกันระหว่างสถาปัตยกรรมของ GPT และ BERT โดยเน้นที่ความเหมือนและความแตกต่าง\",\n",
    "                    \"Choice\" : {\n",
    "                        \"Choice1\" : [\"GPT ใช้การจำลองภาษาที่ปกคลุม\", 0.25],\n",
    "                        \"Choice2\" : [\"BERT ใช้การจำลองภาษาที่ปกคลุม\", 0.25],\n",
    "                        \"Choice3\" : [\"ทั้งสองโมเดลใช้เทคนิค attention mechanism\", 0.25],\n",
    "                        \"Choice4\" : [\"ทั้งหมดข้างต้น\", 1]\n",
    "                            },\n",
    "                    \"Topic\" : \"Transformer Architecture\"\n",
    "                    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 40 column 58 (char 2505)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Text_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(Text)\n",
      "File \u001b[1;32mc:\\Users\\Nattapong\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\Nattapong\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\Nattapong\\anaconda3\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 40 column 58 (char 2505)"
     ]
    }
   ],
   "source": [
    "Text_json = json.loads(Text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
